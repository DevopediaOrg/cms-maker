{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\"><h1>Note: This notebook currently does not work due to absent scraper code</h1></p>\n",
    "\n",
    "# Notebook for Generating Author Traversal Rules from existing Data\n",
    "## Note: This was built to run in Google Colab to utilize the GPU infra\n",
    "\n",
    "## How to use:\n",
    "1. Upload the cms-maker project folder to Google Drive in My Drive\n",
    "2. Open Notebook from Google Drive in colab\n",
    "3. Change runtime to GPU\n",
    "4. Authorize request to google drive when prompted(Doing this allows us to prevent having to copy code into notebook and re-use the existing repo code)\n",
    "5. When complete this will store the results of traversal from data into the drive as prescribed in *validate.py*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "is_colab = False\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  is_colab = True\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "colab_type": "code",
    "id": "N-8lgRDhdIiE",
    "outputId": "cb43c54a-6337-4864-95c2-cf918045ed14"
   },
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    %pwd\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    print(\"Mounted Drive successfully\")\n",
    "    %cd /content\n",
    "    %cd gdrive/My\\ Drive/cms-maker/\n",
    "    %pwd\n",
    "    print(\"Should be in the drive folder of the project, check previous printed line to confirm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "vUEdPuGyfdvB",
    "outputId": "6aa38db4-f4de-424b-e78f-1d8223d2ca18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4==4.8.2\n",
      "  Using cached beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "Collecting certifi==2020.4.5.1\n",
      "  Using cached certifi-2020.4.5.1-py2.py3-none-any.whl (157 kB)\n",
      "Collecting feedparser==5.2.1\n",
      "  Using cached feedparser-5.2.1.tar.bz2 (192 kB)\n",
      "Collecting iso8601==0.1.12\n",
      "  Using cached iso8601-0.1.12-py2.py3-none-any.whl (12 kB)\n",
      "Collecting mysql-connector-python==8.0.19\n",
      "  Using cached mysql_connector_python-8.0.19-py2.py3-none-any.whl (355 kB)\n",
      "Collecting scrapy==2.0.0\n",
      "  Using cached Scrapy-2.0.0-py2.py3-none-any.whl (241 kB)\n",
      "Collecting stanza==1.0.1\n",
      "  Using cached stanza-1.0.1-py3-none-any.whl (193 kB)\n",
      "Collecting tldextract==2.2.2\n",
      "  Using cached tldextract-2.2.2-py2.py3-none-any.whl (48 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.3.1 (from -r ../requirements.txt (line 9)) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 1.4.0, 1.5.0, 1.5.1, 1.6.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==1.3.1 (from -r ../requirements.txt (line 9))\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "xDEuLCo3gu4J",
    "outputId": "23dfb659-7190-4594-d612-63a7723001ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json:   0%|          | 0.00/24.4k [00:00<?, ?B/s]\r",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 7.42MB/s]                    \n",
      "2020-09-05 19:14:43 INFO: Downloading these customized packages for language: en (English)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-09-05 19:14:43 INFO: File exists: /Users/pab/stanza_resources/en/tokenize/ewt.pt.\n",
      "2020-09-05 19:14:43 INFO: Finished downloading models and saved to /Users/pab/stanza_resources.\n",
      "\r",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json:   0%|          | 0.00/24.4k [00:00<?, ?B/s]\r",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 4.20MB/s]                    \n",
      "2020-09-05 19:14:43 INFO: Downloading these customized packages for language: en (English)...\n",
      "===============================\n",
      "| Processor       | Package   |\n",
      "-------------------------------\n",
      "| ner             | ontonotes |\n",
      "| forward_charlm  | 1billion  |\n",
      "| backward_charlm | 1billion  |\n",
      "===============================\n",
      "\n",
      "2020-09-05 19:14:44 INFO: File exists: /Users/pab/stanza_resources/en/ner/ontonotes.pt.\n",
      "2020-09-05 19:14:44 INFO: File exists: /Users/pab/stanza_resources/en/forward_charlm/1billion.pt.\n",
      "2020-09-05 19:14:44 INFO: File exists: /Users/pab/stanza_resources/en/backward_charlm/1billion.pt.\n",
      "2020-09-05 19:14:44 INFO: Finished downloading models and saved to /Users/pab/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "%%python3\n",
    "import stanza\n",
    "stanza.download('en', processors='tokenize')\n",
    "stanza.download('en', processors='ner', package='ontonotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QpSvQoTGfpaS",
    "outputId": "94188e34-eb24-4fcc-e3d7-572dc2532741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"../scripts/validate.py\", line 8, in <module>\n",
      "    from scraper.spiders.contentscraper import ContentScraper\n",
      "ModuleNotFoundError: No module named 'scraper'\n",
      "CPU times: user 19.6 ms, sys: 10.9 ms, total: 30.5 ms\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python3 ../scripts/validate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkO5ILr-uDgn"
   },
   "outputs": [],
   "source": [
    "%ls -lr resources/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cms-maker-traversal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
